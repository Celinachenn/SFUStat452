---
title: "Week 10 Exercises"
author: "Brad McNeney"
date: '2017-11-07'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We will use the `Heart` data set, split into training and 
test sets.

```{r}
uu <- url("http://www-bcf.usc.edu/~gareth/ISL/Heart.csv")
Heart <- read.csv(uu,row.names=1)
Heart <- na.omit(Heart)
dim(Heart) # Train on 2/3, test on 1/3
set.seed(1)
train <- sample(1:nrow(Heart),size=2*nrow(Heart)/3,replace=FALSE)
```


### 1. Exercise with `gam()`

(a) With this exercise I will try to clear up some misinformation
I spread during lecture about the nonparametric effects 
summary from a fitted gam model.
* Fit a `gam()` model for the binary response `AHD` as a function
of the categorical variables `ChestPain` and `Thal` and 
the quantitative variable `MaxHR`. In your model, 
specify a degree 4 smoothing spline for `MaxHR`.
    + Print a model summary. Note the degrees of freedom
    for the nonparametric effects ANOVA. This is the 
    difference between the df for the fitted smoothing spline
    and the df for a **linear** term. The 
    nonparametric test is for testing the non-linear component,
    not the overall effect
    of the smoothing spline, as I said in class. 
    In the ANOVA for the parametric effects,
    the single df test is for the **linear** effect of `MaxHR`.
    + Plot results with standard errors and comment on the fitted terms.

_Solution:_

```{r}
library(gam)
fit1 <- gam(AHD ~ ChestPain+s(MaxHR,4)+Thal, data=Heart,subset=train,family=binomial)
summary(fit1)
plot(fit1,se=TRUE)
```

The output suggests the nonparametric component of the smooth 
for `MaxHR` is not necessary, but that the linear term is.
We can see this in the plot of the `MaxHR` effect.

(b) Fit models with (i) `ChestPain` and `Thal` and a linear effect
for `MaxHR` and (ii) `ChestPain` and `Thal`.
Use the `anova()` function to compute likelihood ratio tests
for (i) the non-linear smooth of `MaxHR` and (ii)
the linear `MaxHR` term. What do you conclude?
    + Note: The tests by `anova()` are likelihood ratio
    tests. The test in part (a) is a slightly diffent 
    kind of test called a score test. We can see that the
    results of the two kinds of tests for the non-linear 
    component of `MaxHR` are nearly identical.
    
_Solution:_

```{r}
fit2<-gam(AHD ~ ChestPain + MaxHR + Thal, data=Heart,subset=train,family=binomial)
fit3 <- gam(AHD ~ ChestPain + Thal, data=Heart,subset=train,family=binomial)
anova(fit3,fit2,fit1)
```

We conclude that a linear term in `MaxHR` is necessary, but that
a non-linear component is not.

(c) Using your prefered model from part (b), predict
`AHD` on the test data with probability 
cut-off 1/2 (i.e., if the probability of AHD is 1/2 
or greater, classify as Yes)
and report the misclassification error rate.


_Solution:_

```{r}
predicted <- predict(fit2,newdata=Heart[-train,],type="response")
predicted <- (predicted>0.5)
trueYes <- Heart[-train,"AHD"]
table(predicted, trueYes)
```

The misclassification rate is (17+10)/(42+17+10+30) = 0.27.

## 2. Decision Trees

(a) Fit a classification tree to the `Heart` data, using 
`ChestPain`, `Thal`, `MaxHR` to classify `AHD`.
Plot the tree and add text to the plot. In your call
to `text()` specify  the argument `pretty=0` to 
see the categories for splits on categorical variables.

_Solution:_

```{r}
library(tree)
t1 <- tree(AHD ~ ChestPain + Thal + MaxHR, data=Heart[train,])
plot(t1)
text(t1,pretty=0,cex=.5)
```

(b) Use cross-validation to select the tree size.

_Solution:_

```{r}
cv.t1 <- cv.tree(t1,FUN=prune.misclass) # use class'n err for CV
plot(cv.t1) # Suggests size 6
```

(c) Use the tree of best size to predict `AHD`, using
a probability cut-off of 1/2. Report 
the misclassification rate and compare to the 
gam/logistic regression approach in part 1.

_Solution:_

```{r}
t1.best <- prune.tree(t1,best=6)
predicted <- predict(t1.best,newdata=Heart[-train,])
predicted <- predicted[,"Yes"] > 0.5
table(predicted, trueYes)
```

The misclassification rate is (17+6)/(46+17+6+30) = 0.23,
which is lower than the 27% misclassfication rate of
logistic regression.

