---
title: "Homework 3 Solutions"
author: "Brad McNeney"
date: '2017-10-04'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,fig.height=4,fig.width=6)
```


## Question 1 (Chapter 6, #8, parts (a)-(e), 6 marks)


(a) (1 mark)
(Note: You should set your random seed, for reproducibility.)

_Solution:_
```{r}
set.seed(1) # or choose your own
n <- 100
X <- rnorm(n)
eps <- rnorm(n)
```

(b) (1 mark)

_Solution:_
```{r}
beta0 <- 1; beta1<-(-1); beta2<-1; beta3<-(-1) # choose your own
Y <- beta0 + beta1*X + beta2*X^2 + beta3*X^3 + eps
```

(c) (3 marks)
For the "best model obtained", you should 
use one that is parsimonious and close to
the consensus best according to the three
selection criteria.

You don't **have** to create a data frame. 
`regsubsets()` can take a design matrix and
response vector, just like `lm.fit()` and 
`glmnet()`. If you do decide to create a data frame,
the following hint may be of use:
```{r}
library(leaps)
pmax <- 10
Xmat <- matrix(NA,nrow=n,ncol=pmax)
for(i in 1:pmax) {
  Xmat[,i] <- X^i
}
colnames(Xmat) <- paste0("X.",1:pmax)
dat <- data.frame(Y,Xmat)
```

_Solution:_

```{r}
fit <- regsubsets(Y~.,data=dat,nvmax=10)
fit.sum <- summary(fit)
```

Your plots should look something 
like the following (1 mark). The 
best model has 3 terms according to BIC and 4 according
to $C_p$ and adjusted $R^2$ (1 mark). You might
say four terms is the consensus best, but 
if we appeal to parsimony we would select three.

```{r,fig.width=8}
par(mfrow=c(1,3))
plot(fit.sum$cp,type="l",ylab=expression(C[p]),
     xlab="Number of Predictors")
plot(fit.sum$bic,type="l",ylab="BIC",
     xlab="Number of Predictors")
plot(fit.sum$adjr2,type="l",ylab="Adjusted R^2",
     xlab="Number of Predictors")
which.min(fit.sum$cp)
which.min(fit.sum$bic)
which.max(fit.sum$adjr2)
```

The coefficients for the model with three terms are as follows
(1 mark). Also OK to report the coefficients from the 
model with four terms.

```{r}
coef(fit,3)
```

(d) (2 marks) 

_Solution:_ Forward selection suggests 4 or 5 terms.
In the interest of parsimony, I  would choose 4, 
but OK to say 5 (1 mark).
Backward selection is a bit erratic, but from the plots 
it looks like 3 or 4 would be fine (1 mark). I've 
printed the models with 4 terms in the 
code chunks that follow.

```{r}
par(mfrow=c(1,3))
fitfwd <- regsubsets(Y~.,data=dat,method="forward",nvmax=10)
fitfwd.sum <- summary(fitfwd)
plot(fitfwd.sum$cp,type="l",ylab=expression(C[p]),
     xlab="Number of Predictors")
plot(fitfwd.sum$bic,type="l",ylab="BIC",
     xlab="Number of Predictors")
plot(fitfwd.sum$adjr2,type="l",ylab="Adjusted R^2",
     xlab="Number of Predictors")
which.min(fitfwd.sum$cp)
which.min(fitfwd.sum$bic)
which.max(fitfwd.sum$adjr2)
coef(fitfwd,4)
#
fitbwd <- regsubsets(Y~.,data=dat,method="backward",nvmax=10)
fitbwd.sum <- summary(fitbwd)
plot(fitbwd.sum$cp,type="l",ylab=expression(C[p]),
     xlab="Number of Predictors")
plot(fitbwd.sum$bic,type="l",ylab="BIC",
     xlab="Number of Predictors")
plot(fitbwd.sum$adjr2,type="l",ylab="Adjusted R^2",
     xlab="Number of Predictors")
which.min(fitbwd.sum$cp)
which.min(fitbwd.sum$bic)
which.max(fitbwd.sum$adjr2)
coef(fitbwd,4)
```


(e) (3 marks)
Use the `cv.glmnet()` function to do the cross-validation
and the `plot()` function to plot it (1 mark). 
I said in class that for "optimal" $\lambda$ you should
use the best by the 1-SE criterion, but also OK to
use the $\lambda$ that minimizes MSE (1 mark).
The final mark is for reporting the coefficients from the
model with the optimal $\lambda$.

```{r}
library(glmnet)
lambdas <- 10^{seq(from=-2,to=5,length=100)}
cv.lafit <- cv.glmnet(Xmat,Y,alpha=1,lambda=lambdas) 
plot(cv.lafit)
lam.best <- cv.lafit$lambda.1se
la.best <- glmnet(Xmat,Y,alpha=1,lambda=lam.best)
coef(la.best)
```



## Question 2 (Ch6, #9, 12 marks)

(a) (0 marks)
To make everyone's results comparable, please
select your test set with the following.
(Note the last-minute change of the following code. You
were asked to standardize **all** variables, including
the response.)

```{r}
library(ISLR)
data(College)
library(dplyr)
# Standardize columns
College <- mutate(College,Private = as.numeric(Private=="Yes"))
College <- data.frame(lapply(College,scale))
dim(College) # 777 rows, use 111 as test
set.seed(1)
testset <- sample(1:777,size=111)
College.test <- College[testset,]
College.train <- College[-testset,]
```

(b) (2 marks)

_Solution:_
One mark for fitting the model, one for calculating the test MSE.

```{r}
fit <- lm(Apps ~ ., data=College.train)
pred <- predict(fit,newdata=College.test)
mean((College.test$Apps - pred)^2)
```


(c) (2 marks)
_Solution:_
One mark for using 
`cv.glmnet()` and
extracting the best $\lambda$ (my preference is
for `lambda.1se`, but `lambda.min` OK too), one mark
for fitting the model with the best $\lambda$ and 
calculating the test MSE.

```{r}
library(glmnet)
X.train <- model.matrix(Apps ~ ., data=College.train)
y.train <- College.train$Apps
cv.fit <- cv.glmnet(X.train,y.train,alpha=0)
lambda.best <- cv.fit$lambda.1se
fit <- glmnet(X.train,y.train,alpha=0,lambda = lambda.best)
X.test <- model.matrix(Apps ~ ., data=College.test)
pred <- predict(fit,newx=X.test)
mean((College.test$Apps - pred)^2)
```

(d) (2 marks)
_Solution:_ The solution is the same as for ridge regression, but
set $\alpha=1$ in `glmnet` to use lasso.

```{r}
cv.fit <- cv.glmnet(X.train,y.train,alpha=1)
lambda.best <- cv.fit$lambda.1se
fit <- glmnet(X.train,y.train,alpha=1,lambda = lambda.best)
pred <- predict(fit,newx=X.test)
mean((College.test$Apps - pred)^2)
```

(e) (2 marks)
_Solution:_
Use the `pcr()` function from the `pls` package,
with option `validation="CV"` for cross-validation (1 mark).
Since we have already scaled our data, you can set the 
`scale` option to `FALSE` (the default), but also
OK to set to `TRUE`.
One mark for selecting a "best" number of components 
(I choose 17, but smaller numbers as low as 7 are OK too),
extracting the predictions on the test data and calculating
the test MSE.

```{r}
library(pls)
set.seed(123)
fit <- pcr(Apps ~ ., data=College.train,validation="CV")
validationplot(fit)
best.ncomp <- 17
pred <- predict(fit,newdata=College.test,ncomp=best.ncomp)
mean((College.test$Apps - pred)^2)
```

(f) (2 marks)
_Solution:_
Use the `plsr()` function from the `pls` package,
with option `validation="CV"` for cross-validation (1 mark).
One mark for selecting a "best" number of components 
(I choose 7),
extracting the predictions on the test data and calculating
the test MSE.

```{r}
set.seed(123)
fit <- plsr(Apps ~ ., data=College.train,validation="CV")
validationplot(fit)
best.ncomp <- 7
pred <- predict(fit,newdata=College.test,ncomp=best.ncomp)
mean((College.test$Apps - pred)^2)
```

(g) (2 marks)
_Solution:_ 
To summarize, the test set errors are about
0.19 for least squares, 0.46 for ridge regression, 0.24 for
the lasso, 0.19 for PCR and 0.19 for PLS.
The question of "How
accurately can we predict the number of admissions"
has no obvious answer, in my opinion. 
I will accept any thoughts you
have for one mark. My thoughts are to compare the test set
errors to the overall variance of the `Apps` variable
in the test data, which is about 2.2 (interestingly,
the variance in the test set is much higher than in 
the training set). The methods we tried in parts (b)-(f)
reduced the variance by about 80% or more, so I would
say yes, we can accurately
predict admissions. 
One mark for saying that there 
are substantial differences in the test errors
of the different methods.
In particular, the shrinkage approaches have poorer test errors than 
the other methods.


## Question 3 (Ch7, #6, 8 marks)

(a) (5 marks)
_Solution:_ Most students probably
modified the code from the 
chapter 7 notes to do the cross-validation. This
code does not included SEs on the CV estimates.
In the following I use the functions from the 
week 9 exercises. (Note: One change to the 
week 9 utility functions is that in the 
`plot.cv.lm()` function I modified the 
calculation of the 1-se df to handle missing values.)

Two marks for the cross-validation (see plot below,
but a list of the CV-based test error estimates
is fine too).
One mark for choosing the degree of the polynomial.
I choose a second-degree polynomial in light
of the SE of the test error estimates, but
you could just make a judgement call from a plot
of test error estimates.
One mark for comparing the CV results
to the model selection based on p-values for
testing individual components. You could 
fit a sequence of nested polynomial models, 
or just base your tests on the coefficients 
table (see below). We would choose a third
degree polynomial. You might mention the
marginally-significant test of the 9th 
degree term, but this was not necessary.
One mark for a plot of the final model (see below).

```{r}
source("week9Util.R")
library(ISLR); data(Wage)
k<-10; nDf <- 10; seed <- 1; cvErrs <- rep(NA,nDf) 
cvDat <- matrix(NA,nrow=nDf,ncol=4)
for(df in 1:nDf) {
  res <- cv.lm(wage ~ poly(age,df),Wage,k,seed)
  merr <- res$meanErr; serr <- res$sdErr
  cvDat[df,] <- c(df,merr,merr-serr,merr+serr)
}
colnames(cvDat) <- c("df","meanErr","lwr","upr")
library(ggplot2)
dfs <- plot.cv.lm(cvDat)
fit <- lm(wage ~ poly(age,dfs$df.1se),data=Wage)
newdat <- data.frame(age=seq(from=min(Wage$age),to=max(Wage$age),length=100))
dfs$df.1se
plotfitWage(fit,Wage,newdat)
round(summary(lm(wage ~ poly(age,nDf),data=Wage))$coef,2)
```


(b) (3 marks)
_Solution:_ Use `cut` instead of `poly` to
construct the predictors. Cutting on a number
of breaks leads to different breakpoints on 
the training and test data, so I had to 
create the vector of breakpoints manually.
I followed the approach in cut of moving the
endpoints outward by 0.1% so that the extreme
values fall in the break intervals. 
To create $k$ stepfunctions requires $k+1$ breaks.

One mark for doing the cross-validation, 
and one for choosing the number of step functions.
I choose 4, but you might have chosen as many as 8.
One mark for plotting the results.

```{r}
nDf <- 10
cvDat <- matrix(NA,nrow=nDf,ncol=4)
ageMin <- min(Wage$age); ageMax <- max(Wage$age)
ageMin <- ageMin - 0.001*(ageMax-ageMin)
ageMax <- ageMax + 0.001*(ageMax-ageMin)
for(df in 2:nDf) {
  ageBreaks <- seq(from=ageMin,to=ageMax,length=(df+1))
  res <- cv.lm(wage ~ cut(age,ageBreaks),Wage,k,seed)
  merr <- res$meanErr; serr <- res$sdErr
  cvDat[df,] <- c(df,merr,merr-serr,merr+serr)
}
colnames(cvDat) <- c("df","meanErr","lwr","upr")
library(ggplot2)
dfs <- plot.cv.lm(cvDat)
ageBreaks <- seq(from=ageMin,to=ageMax,length=(dfs$df.1se+1))
fit <- lm(wage ~ cut(age,ageBreaks),data=Wage)
newdat <- data.frame(age=seq(from=min(Wage$age),to=max(Wage$age),length=100))
dfs$df.1se
plotfitWage(fit,Wage,newdat)
```
