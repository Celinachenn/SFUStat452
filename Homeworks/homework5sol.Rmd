---
title: "Homework 4"
author: "Brad McNeney"
date: '2017-12-01'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Question 1 (Chapter 10, #8, 4 marks)

a) (1 mark)
In what follows I centre and scale the data 
matrix, though it is OK to not do so.

```{r}
pcout <- prcomp(USArrests,scale=TRUE)
pcout$sdev^2/sum(pcout$sdev^2)
```


b) (3 marks)

The loadings, $\phi_{jm}$,
are in the `rotation` component of 
the output of `prcomp()`. 
The denominator of equation (10.8) is
just the sum of squared elements of $X$, `sum(USArrests)`
(1 mark).
For fixed $m$, the numerator of equation (10.8)
can be obtained by looping and summing as you go, or
you can use the short-cut that the term
$\sum_{j=1}^p \phi_{jm} x_{ij}$
is the $i^{th}$ element of $X \phi_m$, where
$\phi_m = (\phi_{1m},\ldots,\phi_{pm})^T$
is column $m$ of the rotation matrix.
Even more of a short-cut is to realize
that the numerator is the column-wise sum
of the squared elements of $X \phi$, where
$\phi$ is the rotation matrix.
(Two marks for getting the numerators.)

```{r}
USArrests.sc <- scale(USArrests)
den <- sum(USArrests.sc^2)
# For first PC
num <- sum((USArrests.sc%*%pcout$rotation[,1])^2)
num/den
# For second PC
num <- sum((USArrests.sc%*%pcout$rotation[,2])^2)
num/den
# Etc. 
# Or, we can get them all in one go with:
nums <- colSums((USArrests.sc%*%pcout$rotation)^2)
nums/den
```


## Question 2 (Chapter 10, #9, 7 marks)

a) (1 mark)
Clustering with complete linkage is
the default in `hclust()` and Euclidean
distance is the default in `dist()`. You
could plot the dendrogram, but this was not necessary.

```{r}
cl <- hclust(dist(USArrests))
# optionally: plot(cl)
```


b) (2 marks)
Cutting is done with `cutree(cl,k=3)` (1 mark).
It is OK to just print the vector of 
cluster memberships returned by `cutree()`, 
though I woud have preferred some sort
of grouping of state names, such as the following.
(One mark for listing the clusters.)

```{r}
cc <- cutree(cl,k=3)
# States in Cluster 1:
names(cc[cc==1])
# States in Cluster 2:
names(cc[cc==2])
# States in Cluster 3:
names(cc[cc==3])
```


c) (2 marks)
The question wasn't clear on whether we 
were to also centre the variables (the
default in `scale()`). To me it makes most
sense to centre and scale, but it is 
OK for you to not centre.
One mark for scaling and one for clustering. Plotting
the resulting dendrogram is optional.

```{r}
USArrests.sc <- scale(USArrests,center=TRUE,scale=TRUE)
cl2 <- hclust(dist(USArrests.sc))
# optionally: plot(cl2)
```


d) (2 marks)
It isn't clear how we are supposed to compare the
clusterings, but given part (b) a reasonable 
choice is to cut the tree from (c) when there
are three clusters and compare these three
clusters to the three
clusters from (b) with a table. 

```{r}
cc2 <- cutree(cl2,k=3)
table(cc,cc2)
```

Note that cluster labels need not have
the same meaning in two clusterings. However,
we can see that what was cluster 1 
in (b) has been more or less split into clusters
1 and 2 in (d), and that clusters 2 and 3 in (b) 
has been more or less merged in (d). 
The bottom line is that the clustering has 
changed quite a bit.
(One mark for a sensible comparison.)

Should variables be scaled?
I think it makes most sense to (centre) and scale
the variables. Otherwise the clustering is 
heavily influenced by the variable(s) with the 
largest variance (in this case, `Assault`) (1 mark).



